# å¯¼å…¥æœºå™¨å­¦ä¹ ç›¸å…³åº“
from sklearn import datasets               # åŠ è½½æ ‡å‡†æ•°æ®é›†
import numpy as np                        # æ•°å€¼è®¡ç®—åº“
from sklearn.model_selection import train_test_split  # æ•°æ®é›†åˆ†å‰²
from sklearn.preprocessing import StandardScaler      # æ•°æ®æ ‡å‡†åŒ–
from sklearn.linear_model import Perceptron           # æ„ŸçŸ¥æœºæ¨¡å‹
from sklearn.metrics import accuracy_score            # å‡†ç¡®ç‡è®¡ç®—
from matplotlib.colors import ListedColormap          # é¢œè‰²æ˜ å°„(åç»­å¯è§†åŒ–ä½¿ç”¨)
import matplotlib.pyplot as plt                       # å¯è§†åŒ–ç»˜å›¾
import matplotlib                                     # åŸºç¡€ç»˜å›¾åº“
from packaging.version import parse as parse_version  # ç‰ˆæœ¬è§£æå·¥å…·(æ›¿ä»£å·²å¼ƒç”¨çš„distutils)
from sklearn.linear_model import LogisticRegression   # é€»è¾‘å›å½’
from sklearn.svm import SVC                           # æ”¯æŒå‘é‡æœº
from sklearn.linear_model import SGDClassifier        # éšæœºæ¢¯åº¦ä¸‹é™åˆ†ç±»
from sklearn.tree import DecisionTreeClassifier       # å†³ç­–æ ‘åˆ†ç±»
from sklearn import tree                              # æ ‘æ¨¡å‹å·¥å…·
from pydotplus import graph_from_dot_data             # å†³ç­–æ ‘å¯è§†åŒ–å·¥å…·
from sklearn.tree import export_graphviz              # å¯¼å‡ºå†³ç­–æ ‘å›¾
from sklearn.ensemble import RandomForestClassifier   # éšæœºæ£®æ—
from sklearn.neighbors import KNeighborsClassifier    # Kè¿‘é‚»ç®—æ³•
from sklearn.multiclass import OneVsRestClassifier

# ä¸­æ–‡æ˜¾ç¤ºé…ç½®(å¿…é¡»è®¾ç½®åœ¨ç»˜å›¾æ“ä½œä¹‹å‰)
plt.rcParams['font.sans-serif'] = ['SimHei']  # æŒ‡å®šé»˜è®¤å­—ä½“
plt.rcParams['axes.unicode_minus'] = False  # è§£å†³è´Ÿå·æ˜¾ç¤ºé—®é¢˜
# æ•°æ®å‡†å¤‡
iris = datasets.load_iris()  # ğŸŸ¢åŠ è½½é¸¢å°¾èŠ±æ•°æ®é›†
X = iris.data[:, [2, 3]]     # ğŸŸ¢ä»…ä½¿ç”¨èŠ±ç“£é•¿åº¦(ç¬¬3åˆ—)å’Œå®½åº¦(ç¬¬4åˆ—)ä½œä¸ºç‰¹å¾
y = iris.target              # ğŸŸ¢ç›®æ ‡å˜é‡(èŠ±çš„ç§ç±»)
# æ•°æ®é›†åˆ†å‰²(åˆ†å±‚æŠ½æ ·)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, 
    test_size=0.3,           # ğŸŸ¢æµ‹è¯•é›†å æ¯”30%
    random_state=1,          # ğŸŸ¢éšæœºç§å­ä¿è¯å¯é‡å¤æ€§
    stratify=y)              # ğŸŸ¢åˆ†å±‚æŠ½æ ·ä¿æŒç±»åˆ«æ¯”ä¾‹
# æ•°æ®æ ‡å‡†åŒ–
scaler = StandardScaler()                     # ğŸŸ¢åˆ›å»ºæ ‡å‡†åŒ–å™¨
X_train_std = scaler.fit_transform(X_train)   # ğŸŸ¢è®­ç»ƒé›†æ‹Ÿåˆå¹¶è½¬æ¢
X_test_std = scaler.transform(X_test)         # ğŸŸ¢æµ‹è¯•é›†ä»…è¿›è¡Œè½¬æ¢

# å¯è§†åŒ–å‡½æ•°(å†³ç­–è¾¹ç•Œç»˜åˆ¶)
def plot_decision_regions(X, y, classifier, test_idx=None, resolution=0.02):
    """ğŸŸ¢å†³ç­–åŒºåŸŸå¯è§†åŒ–å‡½æ•°
    å‚æ•°ï¼š
    X : ç‰¹å¾çŸ©é˜µ
    y : ç›®æ ‡å‘é‡  
    classifier : è®­ç»ƒå¥½çš„åˆ†ç±»å™¨å¯¹è±¡
    test_idx : æµ‹è¯•é›†ç´¢å¼•èŒƒå›´
    resolution : ç½‘æ ¼åˆ†è¾¨ç‡(å€¼è¶Šå°å›¾è¶Šç²¾ç»†)
    """
    # åˆå§‹åŒ–æ ‡è®°å’Œé¢œè‰²
    markers = ('s', 'X', 'o', '^', 'v')  # ğŸŸ¢ä¸åŒç±»åˆ«çš„æ ‡è®°å½¢çŠ¶
    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')  # ğŸŸ¢é¢œè‰²æ–¹æ¡ˆ
    cmap = ListedColormap(colors[:len(np.unique(y))])  # ğŸŸ¢æ ¹æ®ç±»åˆ«æ•°åˆ›å»ºé¢œè‰²æ˜ å°„
    # è®¡ç®—ç½‘æ ¼è¾¹ç•Œ
    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1  # ğŸŸ¢ç‰¹å¾1çš„èŒƒå›´
    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1  # ğŸŸ¢ç‰¹å¾2çš„èŒƒå›´
    # ç”Ÿæˆç½‘æ ¼ç‚¹çŸ©é˜µ
    xx1, xx2 = np.meshgrid(
        np.arange(x1_min, x1_max, resolution),  # ğŸŸ¢xè½´åæ ‡çŸ©é˜µ
        np.arange(x2_min, x2_max, resolution))  # ğŸŸ¢yè½´åæ ‡çŸ©é˜µ
    # é¢„æµ‹æ•´ä¸ªç½‘æ ¼çš„ç±»åˆ«
    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)  # ğŸŸ¢å±•å¹³åé¢„æµ‹
    Z = Z.reshape(xx1.shape)  # ğŸŸ¢æ¢å¤ä¸ºç½‘æ ¼å½¢çŠ¶
    # ç»˜åˆ¶å†³ç­–è¾¹ç•Œ
    plt.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap)  # ğŸŸ¢å¡«å……ç­‰é«˜çº¿
    # ç»˜åˆ¶æ ·æœ¬ç‚¹
    for idx, cl in enumerate(np.unique(y)):
        plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1],
                    alpha=0.8, 
                    c=colors[idx],        # ğŸŸ¢æŒ‰ç±»åˆ«ç€è‰²
                    marker=markers[idx],  # ğŸŸ¢æŒ‰ç±»åˆ«é€‰æ‹©æ ‡è®°
                    label=cl, 
                    edgecolor='black')    # ğŸŸ¢ç‚¹è¾¹ç¼˜é¢œè‰²
    # é«˜äº®æµ‹è¯•é›†æ ·æœ¬
    if test_idx:
        X_test, y_test = X[test_idx, :], y[test_idx]
        plt.scatter(X_test[:, 0], X_test[:, 1],
                    c='none',            # ğŸŸ¢æ— å¡«å……é¢œè‰²
                    edgecolor='black',   # ğŸŸ¢é»‘è‰²è¾¹æ¡†
                    alpha=1.0,           # ğŸŸ¢å®Œå…¨ä¸é€æ˜
                    linewidth=1,         # ğŸŸ¢è¾¹æ¡†çº¿å®½
                    marker='o',          # ğŸŸ¢åœ†å½¢æ ‡è®°
                    s=100,               # ğŸŸ¢æ ‡è®°å°ºå¯¸
                    label='test set')    # ğŸŸ¢å›¾ä¾‹æ ‡ç­¾

#============== æ„ŸçŸ¥æœºåˆ†ç±»å™¨ ==============#
"""
åŸç†è¯´æ˜ï¼š
1. å•å±‚ç¥ç»ç½‘ç»œç»“æ„(è¾“å…¥å±‚+è¾“å‡ºå±‚)
2. æ¿€æ´»å‡½æ•°ï¼šé˜¶è·ƒå‡½æ•°(è¾“å‡º1æˆ–-1)
3. æƒé‡æ›´æ–°è§„åˆ™ï¼šw += Î·(y_i - Å·_i)x_i
4. å­¦ä¹ ç‡Î·æ§åˆ¶å‚æ•°æ›´æ–°æ­¥é•¿
5. é€‚ç”¨äºçº¿æ€§å¯åˆ†æ•°æ®
"""
# æ¨¡å‹è®­ç»ƒ
ppn = Perceptron(
    eta0=0.1,          # ğŸŸ¢å­¦ä¹ ç‡Î·
    random_state=1     # ğŸŸ¢éšæœºç§å­
)
ppn.fit(X_train_std, y_train)  # ğŸŸ¢åœ¨æ ‡å‡†åŒ–æ•°æ®ä¸Šè®­ç»ƒ
# å¯è§†åŒ–å†³ç­–è¾¹ç•Œ
X_combined_std = np.vstack((X_train_std, X_test_std))  # ğŸŸ¢åˆå¹¶è®­ç»ƒé›†å’Œæµ‹è¯•é›†
y_combined = np.hstack((y_train, y_test))              # ğŸŸ¢åˆå¹¶ç›®æ ‡å˜é‡
plot_decision_regions(X_combined_std, y_combined, ppn, test_idx=range(105,150))
plt.title('Perceptron Decision Boundaries')
plt.xlabel('petal length [standardized]')
plt.ylabel('petal width [standardized]')
plt.legend()
plt.show()

#============== é€»è¾‘å›å½’ ==============#
"""
åŸç†è¯´æ˜ï¼š
1. ä½¿ç”¨sigmoidå‡½æ•°å°†çº¿æ€§ç»„åˆæ˜ å°„åˆ°(0,1)æ¦‚ç‡åŒºé—´ï¼šÏƒ(z) = 1/(1+e^-z)
2. æŸå¤±å‡½æ•°ï¼šäº¤å‰ç†µæŸå¤± -Î£[y_i log(Å·_i) + (1-y_i)log(1-Å·_i)]
3. æ­£åˆ™åŒ–å‚æ•°C=1/Î»ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
4. å¤šåˆ†ç±»ç­–ç•¥ï¼šOvR(ä¸€å¯¹å¤š)
"""
# æ¨¡å‹è®­ç»ƒä¸å¯è§†åŒ–
# ğŸŸ¢å¤šåˆ†ç±»ç­–ç•¥ï¼šOne-vs-Rest(æ–°ç‰ˆOneVsRestClassifierå°è£…)
lr = OneVsRestClassifier(LogisticRegression(
    C=100.0,             # ğŸŸ¢æ­£åˆ™åŒ–å¼ºåº¦çš„å€’æ•°(å€¼è¶Šå¤§æ­£åˆ™åŒ–è¶Šå¼±)
    solver='lbfgs',      # ğŸŸ¢ä¼˜åŒ–ç®—æ³•ï¼šæœ‰é™å†…å­˜BFGSç®—æ³•
))
lr.fit(X_train_std, y_train)
plot_decision_regions(X_combined_std, y_combined, lr, test_idx=range(105,150))
plt.title('Logistic Regression Decision Boundaries')
plt.xlabel('petal length [standardized]')
plt.ylabel('petal width [standardized]')
plt.legend()
plt.show()
# æ­£åˆ™åŒ–æ•ˆæœåˆ†æ
weights, params = [], []
for c in np.arange(-5, 5):  # ğŸŸ¢éå†Cå‚æ•°çš„æŒ‡æ•°èŒƒå›´
    lr = LogisticRegression(
        C=10.**c,           # ğŸŸ¢C=10^c 
        solver='lbfgs',
        multi_class='ovr'# ğŸŸ¢å¤šåˆ†ç±»ç­–ç•¥ï¼šOne-vs-Rest(å·²å¼ƒç”¨ï¼Œå»ºè®®ä½¿ç”¨OneVsRestClassifierå°è£…)
)
    lr.fit(X_train_std, y_train)
    weights.append(lr.coef_[1])  # ğŸŸ¢å­˜å‚¨ç¬¬äºŒç±»çš„æƒé‡ç³»æ•°
    params.append(10.**c)
weights = np.array(weights)
plt.plot(params, weights[:, 0], label='petal length')
plt.plot(params, weights[:, 1], linestyle='--', label='petal width')
plt.ylabel('weight coefficient')
plt.xlabel('C')
plt.xscale('log')  # ğŸŸ¢å¯¹æ•°åæ ‡æ˜¾ç¤º
plt.title('Regularization Effect Analysis')
plt.legend()
plt.show()

#============== æ”¯æŒå‘é‡æœº ==============#
"""
åŸç†è¯´æ˜ï¼š
1. æœ€å¤§é—´éš”åˆ†ç±»å™¨ï¼šå¯»æ‰¾æœ€ä¼˜è¶…å¹³é¢ä½¿åˆ†ç±»é—´éš”æœ€å¤§åŒ–
2. æ ¸æŠ€å·§ï¼šé€šè¿‡æ ¸å‡½æ•°å°†æ•°æ®æ˜ å°„åˆ°é«˜ç»´ç©ºé—´
3. æ­£åˆ™åŒ–å‚æ•°Cï¼šæƒè¡¡é—´éš”å¤§å°å’Œåˆ†ç±»é”™è¯¯
4. æ ¸å‡½æ•°ç±»å‹ï¼š
   - linear: çº¿æ€§æ ¸
   - rbf: é«˜æ–¯æ ¸(å¤„ç†éçº¿æ€§å¯åˆ†)
"""
# çº¿æ€§SVM
svm_linear = SVC(
    kernel='linear',   # ğŸŸ¢çº¿æ€§æ ¸å‡½æ•°
    C=1.0,             # ğŸŸ¢æ­£åˆ™åŒ–å‚æ•°
    random_state=1
)
svm_linear.fit(X_train_std, y_train)
plot_decision_regions(X_combined_std, y_combined, svm_linear, test_idx=range(105,150))
plt.title('Linear SVM Decision Boundaries')
plt.xlabel('petal length [standardized]')
plt.ylabel('petal width [standardized]')
plt.legend()
plt.show()

# RBFæ ¸SVM(æ¼”ç¤ºä¸åŒgammaå€¼)
X_xor = np.random.randn(200, 2)  # ğŸŸ¢ç”Ÿæˆéšæœºæ•°æ®é›†
y_xor = np.logical_xor(X_xor[:, 0] > 0, X_xor[:, 1] > 0)  # ğŸŸ¢åˆ›å»ºå¼‚æˆ–å…³ç³»
y_xor = np.where(y_xor, 1, -1)  # ğŸŸ¢è½¬æ¢ä¸º1/-1æ ‡ç­¾
gamma_values = [0.1, 1, 10, 100]  # ğŸŸ¢ä¸åŒgammaå‚æ•°å€¼
for gamma in gamma_values:
    svm_rbf = SVC(
        kernel='rbf',     # ğŸŸ¢å¾„å‘åŸºæ ¸å‡½æ•°
        gamma=gamma,      # ğŸŸ¢æ§åˆ¶å†³ç­–è¾¹ç•Œå¼¯æ›²ç¨‹åº¦
        C=10.0            # ğŸŸ¢æ­£åˆ™åŒ–å‚æ•°
    )
    svm_rbf.fit(X_xor, y_xor)
    plot_decision_regions(X_xor, y_xor, svm_rbf)
    plt.title(f'RBF SVM (gamma={gamma})')
    plt.xlabel('Feature 1')
    plt.ylabel('Feature 2')
    plt.legend()
    plt.show()
    
def gini(p):
    return p * (1 - p) + (1 - p) * (1 - (1 - p))  # ğŸŸ¢åŸºå°¼ä¸çº¯åº¦è®¡ç®—(æ³¨æ„ï¼šæ­¤å®ç°ä¸ºç®€åŒ–ç‰ˆï¼Œæ ‡å‡†å…¬å¼åº”ä¸º 1 - pÂ² - (1-p)Â²)
def entropy(p):
    return - p * np.log2(p) - (1 - p) * np.log2((1 - p))  # ğŸŸ¢ä¿¡æ¯ç†µè®¡ç®—(å•ä½ï¼šæ¯”ç‰¹)
def error(p):
    return 1 - np.max([p, 1 - p])  # ğŸŸ¢åˆ†ç±»é”™è¯¯ç‡è®¡ç®—(å–æœ€å¤§ç±»åˆ«æ¦‚ç‡çš„è¡¥)
# ç”Ÿæˆæ¦‚ç‡å€¼åŒºé—´ [0,1) çš„ç­‰å·®æ•°åˆ—ï¼Œæ­¥é•¿0.01
x = np.arange(0.0, 1.0, 0.01)  # ğŸŸ¢åˆ›å»º0åˆ°1ä¹‹é—´é—´éš”0.01çš„æ•°ç»„(å…±100ä¸ªç‚¹)
# è®¡ç®—ä¸åŒæŒ‡æ ‡å€¼
ent = [entropy(p) if p != 0 else None for p in x]  # ğŸŸ¢å¤„ç†p=0æ—¶çš„logè®¡ç®—å¼‚å¸¸
sc_ent = [e * 0.5 if e else None for e in ent]     # ğŸŸ¢ç†µå€¼ç¼©æ”¾(ç”¨äºå¯è§†åŒ–å¯¹æ¯”)
err = [error(i) for i in x]                        # ğŸŸ¢è®¡ç®—æ‰€æœ‰ç‚¹çš„åˆ†ç±»é”™è¯¯ç‡
# åˆ›å»ºç”»å¸ƒå’Œåæ ‡ç³»
fig = plt.figure()
ax = plt.subplot(111)  # ğŸŸ¢1x1ç½‘æ ¼çš„ç¬¬1ä¸ªå­å›¾(ç»å…¸å•å›¾å¸ƒå±€)
# å¾ªç¯ç»˜åˆ¶å››ç§æŒ‡æ ‡æ›²çº¿
for i, lab, ls, c, in zip(
    [ent, sc_ent, gini(x), err],                   # ğŸŸ¢æ•°æ®é›†åˆ—è¡¨
    ['Entropy', 'Entropy (scaled)',                # ğŸŸ¢å›¾ä¾‹æ ‡ç­¾
     'Gini impurity', 'Misclassification error'],  # ğŸŸ¢(æ³¨æ„ï¼šå®é™…é¢œè‰²é¡ºåºéœ€å¯¹åº”æ•°æ®)
    ['-', '-', '--', '-.'],                        # ğŸŸ¢çº¿å‹ï¼šå®çº¿ã€å®çº¿ã€è™šçº¿ã€ç‚¹åˆ’çº¿
    ['black', 'lightgray', 'red', 'green', 'cyan']):# ğŸŸ¢é¢œè‰²æ–¹æ¡ˆ(å®é™…ä½¿ç”¨å‰4ç§)
    line = ax.plot(x, i, label=lab, linestyle=ls, linewidth=2, color=c)  
# å›¾ä¾‹é…ç½®
ax.legend(loc='upper center', 
         bbox_to_anchor=(0.5, 1.15),  # ğŸŸ¢å°†å›¾ä¾‹å®šä½åœ¨ç”»å¸ƒä¸Šæ–¹(xè½´ä¸­å¿ƒï¼Œyè½´1.15å€é«˜åº¦)
         ncol=5,                       # ğŸŸ¢åˆ†5åˆ—æ˜¾ç¤º(å®é™…4æ¡æ›²çº¿è¶³å¤Ÿå•è¡Œæ˜¾ç¤º)
         fancybox=True,                # ğŸŸ¢åœ†è§’è¾¹æ¡†
         shadow=False)                 # ğŸŸ¢æ— é˜´å½±
# æ·»åŠ å‚è€ƒçº¿
ax.axhline(y=0.5, linewidth=1, color='k', linestyle='--')  # ğŸŸ¢0.5æ°´å¹³è™šçº¿
ax.axhline(y=1.0, linewidth=1, color='k', linestyle='--')  # ğŸŸ¢1.0æ°´å¹³è™šçº¿
# åæ ‡è½´è®¾ç½®
plt.ylim([0, 1.1])          # ğŸŸ¢yè½´èŒƒå›´(ç•™å‡º0.1ç©ºç™½)
plt.xlabel('p(i=1)')        # ğŸŸ¢xè½´æ ‡ç­¾ï¼šç±»åˆ«1çš„æ¦‚ç‡
plt.ylabel('impurity index')# ğŸŸ¢yè½´æ ‡ç­¾ï¼šä¸çº¯åº¦æŒ‡æ ‡
#plt.savefig('images/03_19.png', dpi=300, bbox_inches='tight')  # ğŸŸ¢ä¿å­˜é«˜æ¸…å›¾(æ³¨é‡ŠçŠ¶æ€)
plt.show()




#============== å†³ç­–æ ‘åˆ†ç±»å™¨ ==============#
"""
åŸç†è¯´æ˜ï¼š
1. æ ‘å½¢ç»“æ„æ¨¡å‹ï¼šé€šè¿‡é€’å½’åˆ’åˆ†ç‰¹å¾ç©ºé—´æ„å»ºå†³ç­–è§„åˆ™æ ‘
   - å†…éƒ¨èŠ‚ç‚¹ï¼šç‰¹å¾åˆ¤æ–­æ¡ä»¶(å¦‚petal_length â‰¤ 2.45)
   - å¶èŠ‚ç‚¹ï¼šæœ€ç»ˆåˆ†ç±»ç»“æœ
2. åˆ†è£‚æ ‡å‡†ï¼š
   - åŸºå°¼ä¸çº¯åº¦(Gini impurity)ï¼šè¡¡é‡èŠ‚ç‚¹çº¯åº¦ï¼Œè®¡ç®—å¼ä¸º Gini = 1 - Î£((p|i)Â²)
   - ä¿¡æ¯å¢ç›Š(Information gain)ï¼šåŸºäºä¿¡æ¯ç†µçš„å‡å°‘é‡(æœ¬ç¤ºä¾‹æœªä½¿ç”¨)
3. åœæ­¢æ¡ä»¶æ§åˆ¶ï¼š
   - max_depth=4ï¼šé™åˆ¶æ ‘çš„æœ€å¤§æ·±åº¦é˜²æ­¢è¿‡æ‹Ÿåˆ
   - min_samples_splitï¼šèŠ‚ç‚¹ç»§ç»­åˆ†è£‚çš„æœ€å°æ ·æœ¬æ•°(é»˜è®¤2)
4. ç‰¹æ€§ï¼š
   - éå‚æ•°æ¨¡å‹ï¼šä¸å¯¹æ•°æ®åˆ†å¸ƒåšå‡è®¾
   - ç‰¹å¾é‡è¦æ€§ï¼šè‡ªåŠ¨è¯„ä¼°ç‰¹å¾è´¡çŒ®åº¦
   - å¯è§†åŒ–å‹å¥½ï¼šæ ‘ç»“æ„ç›´è§‚å¯è§£é‡Š
5. æ³¨æ„äº‹é¡¹ï¼š
   - å®¹æ˜“äº§ç”Ÿè¿‡æ‹Ÿåˆ(éœ€é€šè¿‡å‰ªæ/å‚æ•°é™åˆ¶æ§åˆ¶)
   - å¯¹å™ªå£°æ•°æ®æ•æ„Ÿ(éœ€é…åˆæ•°æ®é¢„å¤„ç†)
"""
# åˆ›å»ºå†³ç­–æ ‘åˆ†ç±»å™¨å®ä¾‹
tree_model = DecisionTreeClassifier(
    criterion='gini',    # ğŸŸ¢åˆ†è£‚æ ‡å‡†ï¼šåŸºå°¼ä¸çº¯åº¦(è¡¡é‡èŠ‚ç‚¹çº¯åº¦ï¼Œå€¼è¶Šå°çº¯åº¦è¶Šé«˜)
    max_depth=4,         # ğŸŸ¢æ ‘çš„æœ€å¤§æ·±åº¦(é˜²æ­¢è¿‡æ‹Ÿåˆï¼Œæ§åˆ¶æ¨¡å‹å¤æ‚åº¦)
    random_state=1       # ğŸŸ¢éšæœºç§å­(ä¿è¯å¯å¤ç°æ€§)
)
# æ¨¡å‹è®­ç»ƒ(ä½¿ç”¨åŸå§‹ç‰¹å¾æ•°æ®ï¼Œå†³ç­–æ ‘ä¸éœ€è¦æ ‡å‡†åŒ–)
tree_model.fit(X_train, y_train)  # ğŸŸ¢è¾“å…¥è®­ç»ƒé›†ç‰¹å¾å’Œæ ‡ç­¾
# åˆå¹¶è®­ç»ƒé›†å’Œæµ‹è¯•é›†æ•°æ®(ç”¨äºå®Œæ•´å¯è§†åŒ–)
X_combined = np.vstack((X_train, X_test))  # ğŸŸ¢å‚ç›´å †å ç‰¹å¾çŸ©é˜µ
y_combined = np.hstack((y_train, y_test))  # ğŸŸ¢æ°´å¹³æ‹¼æ¥ç›®æ ‡å‘é‡
# ç»˜åˆ¶å†³ç­–è¾¹ç•Œå¯è§†åŒ–
plot_decision_regions(
    X_combined, y_combined, 
    classifier=tree_model,        # ğŸŸ¢ä¼ å…¥è®­ç»ƒå¥½çš„å†³ç­–æ ‘æ¨¡å‹
    test_idx=range(105, 150)      # ğŸŸ¢é«˜äº®æ˜¾ç¤ºæµ‹è¯•é›†æ ·æœ¬(ç´¢å¼•105-149)
)
# å›¾è¡¨è®¾ç½®
plt.xlabel('petal length [cm]')   # ğŸŸ¢xè½´æ ‡ç­¾(åŸå§‹å•ä½)
plt.ylabel('petal width [cm]')    # ğŸŸ¢yè½´æ ‡ç­¾(åŸå§‹å•ä½)
plt.legend(loc='upper left')      # ğŸŸ¢å›¾ä¾‹ä½ç½®ï¼šå·¦ä¸Šè§’
plt.tight_layout()                # ğŸŸ¢è‡ªåŠ¨è°ƒæ•´å­å›¾å‚æ•°(é¿å…æ ‡ç­¾é‡å )
# plt.savefig('images/03_20.png', dpi=300)  # ğŸŸ¢ä¿å­˜é«˜æ¸…å›¾åƒ(å¯é€‰)
plt.show()                        # ğŸŸ¢æ˜¾ç¤ºå›¾è¡¨
# å¯è§†åŒ–å†³ç­–æ ‘ç»“æ„
tree.plot_tree(tree_model)        # ğŸŸ¢ç»˜åˆ¶æ ‘å½¢ç»“æ„(èŠ‚ç‚¹åŒ…å«åˆ†è£‚æ¡ä»¶å’ŒåŸºå°¼å€¼)
# plt.savefig('images/03_21_1.pdf')  # ğŸŸ¢ä¿å­˜ä¸ºçŸ¢é‡å›¾(å¯é€‰ï¼Œé€‚åˆå°åˆ·)
plt.show()                       # ğŸŸ¢æ˜¾ç¤ºæ ‘å½¢å›¾


#============== éšæœºæ£®æ—åˆ†ç±»å™¨ ==============#
"""
åŸç†è¯´æ˜ï¼š
1. é›†æˆå­¦ä¹ æ–¹æ³•ï¼šé€šè¿‡æ„å»ºå¤šæ£µå†³ç­–æ ‘è¿›è¡ŒæŠ•ç¥¨å†³ç­–
   - æ¯æ£µæ ‘ä½¿ç”¨BootstrapæŠ½æ ·(æœ‰æ”¾å›æŠ½æ ·)è®­ç»ƒ
   - ç‰¹å¾éšæœºé€‰æ‹©ï¼šåˆ†è£‚æ—¶éšæœºé€‰æ‹©éƒ¨åˆ†ç‰¹å¾è¿›è¡Œè€ƒå¯Ÿ
2. æ ¸å¿ƒå‚æ•°ï¼š
   - n_estimators=25ï¼šæ£®æ—ä¸­å†³ç­–æ ‘çš„æ•°é‡
   - criterion='gini'ï¼šèŠ‚ç‚¹åˆ†è£‚æ ‡å‡†ï¼ˆåŸºå°¼ä¸çº¯åº¦ï¼‰
3. ä¼˜åŠ¿ç‰¹æ€§ï¼š
   - å†…ç½®ç‰¹å¾é‡è¦æ€§è¯„ä¼°ï¼šé€šè¿‡ç‰¹å¾çš„å¹³å‡çº¯åº¦æå‡è®¡ç®—
   - è¢‹å¤–æ•°æ®(OOB)è¯„ä¼°ï¼šå¯ç”¨oob_score=Trueå¼€å¯ï¼ˆæœ¬ç¤ºä¾‹æœªä½¿ç”¨ï¼‰
   - å¹¶è¡Œè®¡ç®—ï¼šn_jobså‚æ•°æ§åˆ¶ä½¿ç”¨çš„CPUæ ¸å¿ƒæ•°
4. æ³¨æ„äº‹é¡¹ï¼š
   - æ ‘çš„æ•°é‡è¶Šå¤šé€šå¸¸æ•ˆæœè¶Šå¥½ï¼Œä½†ä¼šå¢åŠ è®¡ç®—æˆæœ¬
   - éšæœºæ€§æ¥æºï¼šæ•°æ®æŠ½æ ·éšæœºæ€§ + ç‰¹å¾é€‰æ‹©éšæœºæ€§
"""
# åˆ›å»ºéšæœºæ£®æ—åˆ†ç±»å™¨å®ä¾‹
forest = RandomForestClassifier(
    criterion='gini',     # ğŸŸ¢åˆ†è£‚æ ‡å‡†ï¼šåŸºå°¼ä¸çº¯åº¦
    n_estimators=20,      # ğŸŸ¢æ£®æ—ä¸­æ ‘çš„æ•°é‡ï¼ˆé»˜è®¤100ï¼‰
    random_state=1,       # ğŸŸ¢æ§åˆ¶BootstrapæŠ½æ ·å’Œç‰¹å¾é€‰æ‹©çš„éšæœºæ€§
    n_jobs=12              # ğŸŸ¢å¹¶è¡Œä½¿ç”¨çš„CPUæ ¸å¿ƒæ•°ï¼ˆ-1è¡¨ç¤ºä½¿ç”¨å…¨éƒ¨ï¼‰
)
# æ¨¡å‹è®­ç»ƒï¼ˆä½¿ç”¨åŸå§‹ç‰¹å¾æ•°æ®ï¼Œä¸å†³ç­–æ ‘ç›¸åŒï¼‰
forest.fit(X_train, y_train)  # ğŸŸ¢è¾“å…¥è®­ç»ƒé›†ç‰¹å¾å’Œæ ‡ç­¾
# å¯è§†åŒ–å†³ç­–è¾¹ç•Œï¼ˆä½¿ç”¨åˆå¹¶åçš„åŸå§‹å°ºåº¦æ•°æ®ï¼‰
plot_decision_regions(
    X_combined, y_combined,
    classifier=forest,        # ğŸŸ¢ä¼ å…¥è®­ç»ƒå¥½çš„éšæœºæ£®æ—æ¨¡å‹
    test_idx=range(105, 150)  # ğŸŸ¢é«˜äº®æ˜¾ç¤ºæµ‹è¯•é›†æ ·æœ¬(ç´¢å¼•105-149)
)
# å›¾è¡¨è®¾ç½®
plt.xlabel('petal length [cm]')  # ğŸŸ¢xè½´æ ‡ç­¾ï¼ˆåŸå§‹å•ä½å˜ç±³ï¼‰
plt.ylabel('petal width [cm]')   # ğŸŸ¢yè½´æ ‡ç­¾ï¼ˆåŸå§‹å•ä½å˜ç±³ï¼‰
plt.legend(loc='upper left')     # ğŸŸ¢å›¾ä¾‹ä½ç½®ï¼šå·¦ä¸Šè§’
plt.tight_layout()               # ğŸŸ¢è‡ªåŠ¨è°ƒæ•´å­å›¾å‚æ•°ï¼ˆé¿å…æ ‡ç­¾é‡å ï¼‰
# plt.savefig('images/03_22.png', dpi=300)  # ğŸŸ¢ä¿å­˜é«˜æ¸…å›¾åƒï¼ˆå¯é€‰ï¼‰
plt.show()                       # ğŸŸ¢æ˜¾ç¤ºå›¾è¡¨



#============== Kè¿‘é‚»åˆ†ç±»å™¨ ==============#
"""
åŸç†è¯´æ˜ï¼š
1. åŸºäºå®ä¾‹çš„å­¦ä¹ ï¼šä¸æ„å»ºæ˜¾å¼æ¨¡å‹ï¼Œé€šè¿‡å­˜å‚¨è®­ç»ƒæ•°æ®è¿›è¡Œé¢„æµ‹
2. è·ç¦»åº¦é‡ï¼šä½¿ç”¨é—µå¯å¤«æ–¯åŸºè·ç¦»ï¼ˆMinkowski distanceï¼‰
   - p=1ï¼šæ›¼å“ˆé¡¿è·ç¦»ï¼ˆL1èŒƒæ•°ï¼‰
   - p=2ï¼šæ¬§æ°è·ç¦»ï¼ˆL2èŒƒæ•°ï¼Œé»˜è®¤ï¼‰
3. å†³ç­–è§„åˆ™ï¼šå¤šæ•°æŠ•ç¥¨æ³•ï¼ˆåˆ†ç±»ï¼‰æˆ–å¹³å‡å€¼ï¼ˆå›å½’ï¼‰
4. æ ¸å¿ƒå‚æ•°ï¼š
   - n_neighbors=5ï¼šè€ƒè™‘æœ€è¿‘5ä¸ªé‚»å±…çš„æ ‡ç­¾
   - weights='uniform'ï¼šé»˜è®¤ç­‰æƒæŠ•ç¥¨ï¼ˆå¯é€‰'distance'æŒ‰è·ç¦»åŠ æƒï¼‰
5. ç‰¹æ€§ï¼š
   - æ— éœ€è®­ç»ƒé˜¶æ®µï¼ˆæƒ°æ€§å­¦ä¹ ï¼‰
   - å¯¹æ•°æ®è§„æ¨¡æ•æ„Ÿï¼ˆéœ€æ ‡å‡†åŒ–å¤„ç†ï¼‰
   - é«˜ç»´æ•°æ®æ•ˆç‡ä¸‹é™ï¼ˆç»´åº¦ç¾éš¾ï¼‰
"""
# åˆ›å»ºKNNåˆ†ç±»å™¨å®ä¾‹ 
knn = KNeighborsClassifier(
    n_neighbors=5,   # ğŸŸ¢è¿‘é‚»æ•°é‡ï¼ˆå¥‡æ•°å¯é¿å…å¹³ç¥¨ï¼‰
    p=2,             # ğŸŸ¢è·ç¦»åº¦é‡å‚æ•°ï¼ˆ2=æ¬§æ°è·ç¦»ï¼‰
    metric='minkowski' # ğŸŸ¢é—µå¯å¤«æ–¯åŸºè·ç¦»ï¼ˆp=2æ—¶ç­‰æ•ˆæ¬§æ°è·ç¦»ï¼‰
)
# æ¨¡å‹è®­ç»ƒï¼ˆå®é™…åªéœ€å­˜å‚¨æ ‡å‡†åŒ–åçš„æ•°æ®ï¼‰
knn.fit(X_train_std, y_train)  # ğŸŸ¢è¾“å…¥æ ‡å‡†åŒ–åçš„è®­ç»ƒæ•°æ®
# å¯è§†åŒ–å†³ç­–è¾¹ç•Œï¼ˆä½¿ç”¨æ ‡å‡†åŒ–åçš„åˆå¹¶æ•°æ®ï¼‰
plot_decision_regions(
    X_combined_std,           # ğŸŸ¢æ ‡å‡†åŒ–åçš„ç‰¹å¾çŸ©é˜µ
    y_combined,               # ğŸŸ¢åˆå¹¶åçš„ç›®æ ‡å˜é‡
    classifier=knn,           # ğŸŸ¢ä¼ å…¥è®­ç»ƒå¥½çš„KNNæ¨¡å‹
    test_idx=range(105, 150)  # ğŸŸ¢é«˜äº®æµ‹è¯•é›†æ ·æœ¬(ç´¢å¼•105-149)
)
# å›¾è¡¨è®¾ç½®
plt.xlabel('petal length [standardized]')  # ğŸŸ¢xè½´æ ‡ç­¾ï¼ˆæ ‡å‡†åŒ–å•ä½ï¼‰
plt.ylabel('petal width [standardized]')   # ğŸŸ¢yè½´æ ‡ç­¾ï¼ˆæ ‡å‡†åŒ–å•ä½ï¼‰
plt.legend(loc='upper left')               # ğŸŸ¢å›¾ä¾‹ä½ç½®ï¼šå·¦ä¸Šè§’
plt.tight_layout()                         # ğŸŸ¢è‡ªåŠ¨è°ƒæ•´å¸ƒå±€ï¼ˆé˜²æ­¢æ ‡ç­¾é‡å ï¼‰
# plt.savefig('images/03_24.png', dpi=300)  # ğŸŸ¢ä¿å­˜é«˜æ¸…å›¾åƒï¼ˆå¯é€‰ï¼‰
plt.show()    